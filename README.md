# Лабораторная работа №2: Коллективные операции MPI. Скалярное произведение и умножение транспонированной матрицы на вектор

## Цель работы
Закрепить навыки работы с коллективными операциями MPI (Scatter, Gather, Reduce, Allreduce). Реализовать параллельные алгоритмы для вычисления скалярного произведения векторов и умножения транспонированной матрицы на вектор. Подготовить компоненты для реализации метода сопряженных градиентов.

### Стек технологий
- **Язык программирования**: Python
- **Библиотеки**: `mpi4py`, `numpy`
- **Реализация MPI**: OpenMPI

## Тестовые данные
- **Файлы**:
  - `in.dat`: Содержит размеры M и N (50 50).
  - `AData.dat`: Матрица A размером 50 × 50.
  - `xData.dat`: Вектор x длиной 50.

## Этапы реализации

### Этап 0: Генерация тестовых данных
- Реализована в файле `generate.py`.
- Создаёт файлы `in.dat`, `AData.dat`, `xData.dat` с заданными размерами (M = 50, N = 50).
- Использует `numpy.random.rand` для генерации матрицы A и вектора x.

### Этап 1: Параллельное вычисление скалярного произведения
- Реализована в файле `dot_product.py`.
- Выполняет параллельное вычисление скалярного произведения вектора на себя с использованием операций MPI.Scatterv, MPI.Reduce и MPI.Allreduce.
- Процесс 0 генерирует вектор и выполняет последовательное вычисление для верификации.

### Этап 2: Параллельное умножение транспонированной матрицы на вектор
- Реализована в файле `matrix_vector.py`.
- Выполняет параллельное вычисление произведения транспонированной матрицы A на вектор x с использованием операций MPI.Scatterv и MPI.Reduce.
- Процесс 0 считывает данные из файлов и выполняет последовательное вычисление для верификации.

## Результаты
### Этап 1: Скалярное произведение
- **Входные данные**: Вектор `a` длиной `M = 10`.
- **Вывод**:
  - Последовательное скалярное произведение: `385.0`
  - Параллельное (Reduce): `385.0`
  - Параллельное (Allreduce) на всех процессах (0, 1, 2, 3): `385.0`
- **Анализ**: Результаты совпадают, что подтверждает корректность распределения данных и редукции.
<img width="1104" height="440" alt="image" src="https://github.com/user-attachments/assets/a568c686-9a3f-4de1-8954-5c2398d37884" />


### Этап 2: Умножение транспонированной матрицы на вектор
- **Входные данные**: Матрица `A` размером `50 × 50`, вектор `x` длиной `50`.
- **Вывод**:
  - Последовательный результат: Вектор длиной 50 (например, `[12.7156593, 12.05625761, ...]`)
  - Параллельный результат: Тот же вектор длиной 50, совпадающий с последовательным.
- **Анализ**: Совпадение результатов подтверждает правильность алгоритма распределения и суммирования.
<img width="1105" height="842" alt="image" src="https://github.com/user-attachments/assets/bf9cd924-2c28-4362-94c9-8c9913a6aa16" />



## Анализ результатов
### Описание реализованных алгоритмов
- **Скалярное произведение (`dot_product.py`)**: Вектор разбивается на блоки с помощью MPI.Scatterv, каждый процесс вычисляет локальное скалярное произведение, а затем результаты суммируются с использованием MPI.Reduce (на процессе 0) или MPI.Allreduce (на всех процессах).
- **Умножение транспонированной матрицы на вектор (`matrix_vector.py`)**: Матрица A и вектор x разбиваются на горизонтальные полосы с помощью MPI.Scatterv, каждый процесс вычисляет локальное произведение A_part.T @ x_part, а итоговый вектор собирается с помощью MPI.Reduce с операцией MPI.SUM на процессе 0.

### Обоснование выбора функций MPI
- **MPI.Scatterv**: Используется для неравномерного распределения данных (матрицы и вектора) между процессами, учитывая остаток от деления M на количество процессов.
- **MPI.Reduce**: Применяется для суммирования частичных результатов на процессе 0, что оптимально для задач, где итог нужен только на одном процессе.
- **MPI.Allreduce**: Используется для получения общего результата на всех процессах, что удобно для верификации и отладки.

### Сравнение времени выполнения
- **Скалярное произведение**:
  - Последовательное время: ~0.0000465 с (среднее из 0.000058 и 0.000035).
  - Параллельное время (Reduce): 0.000109 с (2 процесса), 0.000125 с (4 процесса).
  - Параллельное время (Allreduce): 0.00003 с (2 процесса), 0.00004425 с (4 процесса).
  - **Анализ**: Ускорение для `Allreduce` достигает ~1.55x (0.0000465 / 0.00003) при 2 процессах и ~1.05x (0.0000465 / 0.00004425) при 4 процессах. Для `Reduce` ускорение отсутствует (0.0000465 / 0.000109 ≈ 0.43 и 0.0000465 / 0.000125 ≈ 0.37), что связано с накладными расходами MPI, превышающими выгоду при малом размере вектора (M = 10).
- **Умножение транспонированной матрицы**:
  - Последовательное время: ~0.016744 с (среднее из 0.014330 и 0.019157).
  - Параллельное время: 0.000413 с (2 процесса), 0.000353 с (4 процесса).
  - **Анализ**: Ускорение составляет ~40.54x (0.016744 / 0.000413) при 2 процессах и ~47.4x (0.016744 / 0.000353) при 4 процессах. Значительное ускорение обусловлено эффективным распределением вычислений для матрицы 50×50, хотя рост ускорения замедляется из-за коммуникационных накладных расходов при увеличении числа процессов.

<img width="640" height="480" alt="speedup_plot" src="https://github.com/user-attachments/assets/3fe6e7a1-74db-44f3-bcac-b3c804b039ed" />


### Почему MPI.Reduce с MPI.SUM, а не MPI.Gatherv
MPI.Reduce с MPI.SUM используется для умножения транспонированной матрицы, так как итоговый вектор b формируется как сумма векторов b_temp со всех процессов. Это позволяет выполнять операцию суммирования непосредственно во время передачи данных, минимизируя объем передаваемой информации (только один вектор длиной N на процессе 0). MPI.Gatherv собирает все векторы b_temp на процесс 0, что требует передачи большего объема данных (M × N элементов вместо N), а затем суммирование выполняется последовательно, увеличивая нагрузку на процесс 0 и время выполнения.

## Выводы
Параллельные реализации скалярного произведения и умножения транспонированной матрицы на вектор с использованием MPI демонстрируют различную эффективность в зависимости от размера данных и числа процессов. Для умножения транспонированной матрицы на вектор (размер 50×50) наблюдается значительное ускорение (до 47.4x при 4 процессах), что подтверждает эффективность параллелизации для задач с большими матрицами. Для скалярного произведения (вектор длиной 10) ускорение минимально или отсутствует (до 1.55x для Allreduce при 2 процессах), из-за доминирования накладных расходов MPI над вычислительной нагрузкой. Наилучшие результаты достигаются при 2-4 процессах, при дальнейшем увеличении числа процессов эффект снижается. Для повышения эффективности скалярного произведения рекомендуется увеличить размер вектора, чтобы вычисления стали доминировать над коммуникациями. Полученные результаты подтверждают применимость методов для задач с большими объёмами данных.
