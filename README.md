# Лабораторная работа №0202_lab: Коллективные операции MPI. Скалярное произведение и умножение транспонированной матрицы на вектор

## Цель работы
Закрепить навыки работы с коллективными операциями MPI (Scatter, Gather, Reduce, Allreduce). Реализовать параллельные алгоритмы для вычисления скалярного произведения векторов и умножения транспонированной матрицы на вектор. Подготовить компоненты для реализации метода сопряженных градиентов.

### Стек технологий
- **Язык программирования**: Python
- **Библиотеки**: `mpi4py`, `numpy`
- **Реализация MPI**: OpenMPI или MPICH
- **Операционная система**: WSL2 на Windows

## Этапы реализации

### Этап 1: Параллельное вычисление скалярного произведения
- Реализована в файле `dot_product.py`.
- Выполняет вычисление скалярного произведения вектора на себя (`a * a`) с использованием `MPI.Scatterv`, `MPI.Reduce` и `MPI.Allreduce`.
- Требования:
  - Инициализация MPI и получение ранга процесса и общего числа процессов.
  - Генерация вектора `a` длиной `M` процессом 0 с использованием `numpy.arange()`.
  - Распределение вектора `a` с помощью `MPI.Scatterv` с учетом неравномерного деления (массивы `rcounts` и `displs`).
  - Локальное вычисление `local_dot = numpy.dot(a_part, a_part)` на каждом процессе.
  - Глобальная редукция:
    - Вариант А: `MPI.Reduce` с операцией `MPI.SUM` для сбора результата на процессе 0.
    - Вариант Б: `MPI.Allreduce` с той же операцией для получения результата на всех процессах.
  - Верификация: Процесс 0 вычисляет скалярное произведение последовательно с помощью `numpy.dot(a, a)` и сравнивает с параллельным результатом.

### Этап 2: Параллельное умножение транспонированной матрицы на вектор
- Реализована в файле `matrix_vector.py`.
- Выполняет вычисление `b = A.T @ x` с использованием `MPI.Scatterv` и `MPI.Reduce`.
- Требования:
  - Чтение параметров `M` и `N` из файла `in.dat` процессом 0 с использованием `MPI.Bcast`.
  - Чтение матрицы `A` (размером `M × N`) и вектора `x` (длиной `M`) из файлов `AData.dat` и `xData.dat`.
  - Распределение матрицы `A` и вектора `x` с помощью `MPI.Scatterv` с учетом неравномерного деления (массивы `rcounts` и `displs`).
  - Локальное вычисление `b_temp = numpy.dot(A_part.T, x_part)` на каждом процессе.
  - Сбор результатов в вектор `b` на процессе 0 с помощью `MPI.Reduce` с операцией `MPI.SUM`.
  - Верификация: Процесс 0 вычисляет результат последовательно с помощью `numpy.dot(A.T, x)` и сравнивает с параллельным результатом.

## Тестовые данные
- **Файлы**:
  - `in.dat`: Содержит размеры `M` и `N` (например, `50\n50`).
  - `AData.dat`: Матрица `A` размером `M × N`.
  - `xData.dat`: Вектор `x` длиной `M`.
- **Генерация тестовых данных**: Используйте скрипт `generate.py` для создания файлов с заданными размерами.

## Результаты
### Этап 1: Скалярное произведение
- **Входные данные**: Вектор `a` длиной `M = 10`.
- **Вывод**:
  - Последовательное скалярное произведение: `385.0`
  - Параллельное (Reduce): `385.0`
  - Параллельное (Allreduce) на всех процессах (0, 1, 2, 3): `385.0`
- **Анализ**: Результаты совпадают, что подтверждает корректность распределения данных и редукции.

### Этап 2: Умножение транспонированной матрицы на вектор
- **Входные данные**: Матрица `A` размером `50 × 50`, вектор `x` длиной `50`.
- **Вывод**:
  - Последовательный результат: Вектор длиной 50 (например, `[12.7156593, 12.05625761, ...]`)
  - Параллельный результат: Тот же вектор длиной 50, совпадающий с последовательным.
- **Анализ**: Совпадение результатов подтверждает правильность алгоритма распределения и суммирования.


## Выводы
- Реализованы параллельные алгоритмы для скалярного произведения и умножения транспонированной матрицы на вектор с использованием коллективных операций MPI.
- Алгоритмы корректно обрабатывают неравномерное деление данных благодаря `rcounts` и `displs`.
- Производительность зависит от размера данных; для небольших `M` и `N` накладные расходы MPI могут превышать выгоду от параллелизма. Тестирование на больших данных (например, `M = 1000`, `N = 1000`) покажет более заметное ускорение.
